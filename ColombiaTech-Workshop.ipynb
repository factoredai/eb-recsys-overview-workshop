{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0da11b5",
   "metadata": {},
   "source": [
    "![Factored Banner](images/Factored_Logo_Profile_Asset_Cover-.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb9e7db",
   "metadata": {},
   "source": [
    "---\n",
    "# 🇨🇴 **ColombiaTechFest – Workshop – Factored**  \n",
    "## 🚀 **Beyond Matrix Factorization: Deep RecSys Architectures in Action**\n",
    "\n",
    "Recommender systems are one of the 💎 **most impactful applications of machine learning** in business today. They help people navigate and interact with the endless variety of products and services companies offer — from 🎵 **Spotify** playlists tailored to your mood, to 🍿 **Netflix** suggesting your next binge-worthy series.  \n",
    "\n",
    "They’re everywhere in our daily lives — and when used effectively, they can **boost engagement, satisfaction, and business growth** 📈.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ **What’s in this workshop?**\n",
    "\n",
    "In this **hands-on session**, we’ll start with **traditional approaches** like collaborative filtering and **Matrix Factorization**, and then step into the world of **modern deep learning architectures** that power today’s most sophisticated platforms.  \n",
    "\n",
    "You’ll leave with:\n",
    "- 🧠 **Conceptual understanding** of core and advanced models.\n",
    "- 💻 **Code examples** you can run and adapt.\n",
    "- 🗺️ **Guidance** on when to use each approach.\n",
    "\n",
    "--- \n",
    "## Workshop Repository: https://github.com/factoredai/eb-recsys-overview-workshop\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242309b5",
   "metadata": {},
   "source": [
    "## **Quick Overview: Traditional Recommenders**\n",
    "\n",
    "### 🧩 **From Matrix Factorization to Factorization Machines**\n",
    "\n",
    "**🔍 Matrix Factorization Recap**  \n",
    "Matrix Factorization (MF) is a foundation of collaborative filtering. It learns:\n",
    "- 👤 **User vectors** → latent preferences.\n",
    "- 🎯 **Item vectors** → latent attributes.\n",
    "\n",
    "Prediction is the dot product of the two:\n",
    "$$\n",
    "\\hat{r}_{ui} = \\mathbf{p}_u^\\top \\mathbf{q}_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{p}_u$ = latent vector for user $u$\n",
    "- $\\mathbf{q}_i$ = latent vector for item $i$\n",
    "\n",
    "✅ **Strengths**: Great for uncovering hidden patterns in sparse user–item data.  \n",
    "⚠️ **Limitations**: Only models interactions between *user ID* and *item ID*. No easy way to add context or side features.\n",
    "\n",
    "![Matrix Factorization](images/Matrix_Architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 **Factorization Machines (FMs)**\n",
    "\n",
    "**💡 Concept & Motivation**  \n",
    "Factorization Machines extend MF to model interactions between **any pair of features** — not just users and items. That means you can mix:\n",
    "- 🆔 IDs (users, items)\n",
    "- 🏷️ Metadata (genres, categories)\n",
    "- ⏱️ Context (time, location)\n",
    "\n",
    "Perfect for **sparse, high‑dimensional data** like CTR prediction or modern recommender systems.\n",
    "\n",
    "They bridge the gap between:\n",
    "- 📏 **Linear models** → handle individual features well.\n",
    "- 🔄 **Nonlinear models** → capture complex feature interactions.\n",
    "\n",
    "**📐 Mathematical Formulation**  \n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^{n} w_i x_i\n",
    "+ \\sum_{i=1}^{n}\\sum_{j=i+1}^{n} \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x}$ = feature vector (user ID, item ID, side features)\n",
    "- $w_0$ = global bias\n",
    "- $w_i$ = weight for feature $i$\n",
    "- $\\mathbf{v}_i \\in \\mathbb{R}^k$ = latent vector for feature $i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e92f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "import functions_workshop as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61d4de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 📝 Task 1 to 3 – Preparing the Dataset\n",
    "\n",
    "Before we dive into modeling, let’s make sure our data is in the right shape.  \n",
    "In this task, you’ll load and prepare the **MovieLens dataset** so it’s ready for training recommender models.\n",
    "\n",
    "#### ✅ Steps\n",
    "\n",
    "1. **Load the Data**  \n",
    "   - Read the three CSV files into separate **Pandas DataFrames**.\n",
    "\n",
    "2. **Merge the Tables**  \n",
    "   - Combine them so that each row contains the following fields:  \n",
    "     - `userId`  \n",
    "     - `movieId`  \n",
    "     - `rating`  \n",
    "\n",
    "3. **Encode IDs**  \n",
    "   - Reindex `userId` and `movieId` so they become **consecutive integers starting from 0**.  \n",
    "   - You can use `LabelEncoder` or create a manual mapping with Pandas.\n",
    "\n",
    "---\n",
    "\n",
    "💡 *Hint*: This preprocessing step ensures that both our matrix factorization and deep learning models can handle users and items efficiently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b52944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TO DO 1 ===\n",
    "# Load the MovieLens dataset from the CSV files into Pandas DataFrames\n",
    "# Complete the function load_movielens_data in task1.py\n",
    "# The function should return three DataFrames: users, movies, and ratings\n",
    "from src.task1 import load_movielens_data\n",
    "\n",
    "users, movies, ratings = load_movielens_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c543277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TO DO 2 ===\n",
    "# Merge the three DataFrames into one called `data`\n",
    "# The final DataFrame should contain: userId, movieId, and rating\n",
    "# Complete the function merge_data in task2.py\n",
    "# The function should return a DataFrame with the merged data\n",
    "from src.task2 import merge_data\n",
    "data = merge_data(users, movies, ratings)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e88f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TO DO 3 ===\n",
    "# Encode userId and movieId as integer indices for embeddings/one-hot encoding\n",
    "# Add two new columns to `data`: \n",
    "#   - u_idx (encoded userId)\n",
    "#   - m_idx (encoded movieId)\n",
    "#\n",
    "# Complete the function encode_user_movie_ids in task3.py\n",
    "# The function should return the DataFrame with the new columns\n",
    "\n",
    "from src.task3 import encode_user_movie_ids\n",
    "data = encode_user_movie_ids(data)\n",
    "print(data[['u_idx', 'm_idx']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e5423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get target variable and number of users/items\n",
    "y = data[\"Rating\"].astype(float).values\n",
    "n_users = data[\"u_idx\"].nunique()\n",
    "n_items = data[\"m_idx\"].nunique()\n",
    "\n",
    "# genres split (for models that use side features)\n",
    "data[\"Genres_list\"] = data[\"Genres\"].fillna(\"(no genres listed)\").str.split(\"|\")\n",
    "id_to_movie = data[['m_idx', 'Title']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(data[['u_idx', 'm_idx', 'Rating','Genres_list','Gender','Age']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63129cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of Factorizacion Machine Model\n",
    "class FactorizationMachine:\n",
    "    \"\"\"\n",
    "    A simple Factorization Machine for regression (e.g., rating prediction)\n",
    "    trained with stochastic gradient descent (SGD).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_features : int\n",
    "        Total number of input features (columns in X).\n",
    "    k : int, default=10\n",
    "        Number of latent factors for modeling pairwise interactions.\n",
    "    learning_rate : float, default=0.01\n",
    "        SGD step size.\n",
    "    n_iter : int, default=100\n",
    "        Number of passes (epochs) over the training data.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Assumes X is a scipy.sparse CSR matrix for efficiency.\n",
    "    - Uses squared error loss: (y - y_hat)^2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, k=10, learning_rate=0.01, n_iter=100):\n",
    "        self.k = k\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "        # Model parameters:\n",
    "        self.w0 = 0.0                          # global bias\n",
    "        self.W = np.zeros(n_features)          # linear weights\n",
    "        # latent factors initialized small (Gaussian)\n",
    "        self.V = np.random.normal(scale=0.01, size=(n_features, k))\n",
    "\n",
    "    def _predict_instance(self, x):\n",
    "        \"\"\"\n",
    "        Predict a single instance.\n",
    "        x: 1xN sparse row (CSR format expected)\n",
    "        \"\"\"\n",
    "        # Linear term: w0 + x · W\n",
    "        linear = self.w0 + x.dot(self.W)  # returns a (1,) ndarray\n",
    "\n",
    "        # Interaction term using the FM identity:\n",
    "        # 0.5 * [ (xV)^2 - (x^2)(V^2) ] summed over features and factors\n",
    "        # x.dot(self.V) -> shape (1, k)\n",
    "        xv = x.dot(self.V)               # (1, k)\n",
    "        xv_sq = np.sum(xv**2)            # scalar\n",
    "        # (x.multiply(x)) keeps sparsity; (V**2) is dense (N,k); result is (1,k)\n",
    "        x_sq_v_sq = (x.multiply(x)).dot(self.V**2)\n",
    "        x_sq_v_sq_sum = np.sum(x_sq_v_sq)\n",
    "\n",
    "        interactions = 0.5 * (xv_sq - x_sq_v_sq_sum)\n",
    "\n",
    "        # Both linear and interactions are scalars now\n",
    "        return float(linear + interactions)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Vectorized predict over all rows for convenience.\n",
    "        Loops over rows to reuse _predict_instance (works fine with sparse).\n",
    "        \"\"\"\n",
    "        return np.array([self._predict_instance(X[i]) for i in range(X.shape[0])])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train with simple SGD over epochs.\n",
    "\n",
    "        X: CSR matrix of shape (n_samples, n_features)\n",
    "        y: array of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        assert isinstance(X, csr_matrix), \"Use a CSR sparse matrix for X.\"\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            for i in range(X.shape[0]):\n",
    "                x_i = X[i]                    # 1xN CSR sparse row\n",
    "                y_hat = self._predict_instance(x_i)\n",
    "                error = y[i] - y_hat          # residual\n",
    "\n",
    "                # === Update w0 (scalar) ===\n",
    "                self.w0 += self.lr * error\n",
    "\n",
    "                # === Update W (linear weights) ===\n",
    "                # Only update positions where x_i is non-zero for sparsity efficiency\n",
    "                # x_i.indices -> non-zero column indices\n",
    "                # x_i.data    -> non-zero values at those indices\n",
    "                for idx, val in zip(x_i.indices, x_i.data):\n",
    "                    self.W[idx] += self.lr * error * val\n",
    "\n",
    "                # === Update V (latent factors) ===\n",
    "                # For each factor f, use the FM gradient:\n",
    "                # dL/dV[j,f] = -error * ( x_j * ( sum_l x_l * V[l,f] - V[j,f] * x_j ) )\n",
    "                # We compute xV[:, f] once, then update only non-zero j.\n",
    "                xV = x_i.dot(self.V)  # shape (1, k)\n",
    "                for f in range(self.k):\n",
    "                    xV_f = xV[0, f]   # scalar\n",
    "                    for j, xj in zip(x_i.indices, x_i.data):\n",
    "                        v_jf = self.V[j, f]\n",
    "                        grad = error * (xj * (xV_f - v_jf * xj))\n",
    "                        self.V[j, f] += self.lr * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use one hot encoding to create user and item features, also create sparse matrix for Factorization Machine\n",
    "ohe_u = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "ohe_m = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "X_u = ohe_u.fit_transform(data[[\"u_idx\"]])     # (n, n_users)\n",
    "X_m = ohe_m.fit_transform(data[[\"m_idx\"]])     # (n, n_items)\n",
    "X_fm = hstack([X_u, X_m]).tocsr()            # (n, n_users+n_items)\n",
    "\n",
    "### Train the Factorization Machine model\n",
    "fm = FactorizationMachine(n_features=X_fm.shape[1], k=3, learning_rate=0.1, n_iter=1)\n",
    "fm.fit(X_fm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get recommendations for a specific user\n",
    "user_idx = 10  # encoded user\n",
    "n_items = len(ohe_m.categories_[0])  # number of encoded items\n",
    "\n",
    "# Build features for (user_idx, each item)\n",
    "u_vecs = ohe_u.transform([[user_idx]] * n_items)  # repeat user row\n",
    "m_vecs = ohe_m.transform([[i] for i in range(n_items)])\n",
    "\n",
    "X_all_items = hstack([u_vecs, m_vecs]).tocsr()\n",
    "\n",
    "# Predict ratings\n",
    "preds = fm.predict(X_all_items)\n",
    "\n",
    "# Rank items by predicted score\n",
    "top_n = 5\n",
    "top_items = np.argsort(preds)[::-1][:top_n]\n",
    "\n",
    "### Get original movie titles for the top recommended items\n",
    "top_item_ids = ohe_m.inverse_transform(m_vecs[top_items])\n",
    "for idx in top_item_ids:\n",
    "    movie_rec = id_to_movie[id_to_movie.m_idx == idx[0]]['Title'].values[0] if idx in id_to_movie.m_idx.values else \"Unknown\"\n",
    "    print(f\"Recommended Movie ID: {idx}, Title: {movie_rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f963e1d9",
   "metadata": {},
   "source": [
    "# **Deep RecSys Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcc1949",
   "metadata": {},
   "source": [
    "## **Part 2 – Wide & Deep Learning (WDL)**  \n",
    "\n",
    "**💡 Concept & Motivation**  \n",
    "- Introduced by Google in 2016 for **large-scale recommendations** and **click-through rate (CTR) prediction**.  \n",
    "- **Wide** 🏎️ = memorization of **explicit feature interactions** (fast, rule-based learning).  \n",
    "- **Deep** 🧠 = generalization via **embeddings** and **neural networks** (learn hidden patterns).  \n",
    "\n",
    "**🏗️ Architecture**  \n",
    "- **Wide branch**: Generalized Linear Model (GLM) with original features + cross features.  \n",
    "- **Deep branch**: Embedding layers for categorical features + dense layers for non-linear transformations.  \n",
    "- Outputs from both branches are **concatenated** and fed into a final prediction layer.  \n",
    "\n",
    "![Wide & Deep Architecture](images/Wide_Deep_Models_Architecture.png)  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 **Why Both Components Are Necessary**\n",
    "\n",
    "#### 1️⃣ Complementary Strengths\n",
    "| Component | Best At... | How It Works |\n",
    "| --------- | ---------- | ------------ |\n",
    "| **Wide**  | Frequent, memorized patterns | Uses explicit cross-features to memorize known rules |\n",
    "| **Deep**  | Unseen or complex patterns   | Uses learned embeddings + nonlinearities to generalize |\n",
    "\n",
    "✅ **Wide** → memorizes co-occurrence rules.  \n",
    "✅ **Deep** → generalizes to rare or never-before-seen combinations.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2️⃣ Cold-Start & Long-Tail Handling\n",
    "- **Wide branch** handles \"hot\" user–item combos efficiently.  \n",
    "- **Deep branch** helps with cold-start problems by using **shared embeddings** and generalizing from similar known items/users.  \n",
    "- The deep path can make a prediction for an unseen pair if their embeddings are close to known patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3️⃣ Balanced Recommendations\n",
    "- **Wide** keeps strong priors: “User 123 always buys Item 456.”  \n",
    "- **Deep** promotes diversity and serendipity by exploring subtle or novel associations.  \n",
    "- This balance helps avoid overfitting to popular items while still recommending relevant content.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4️⃣ Scalability & Performance\n",
    "- **Wide branch** → sparse and fast (especially at inference time).  \n",
    "- **Deep branch** → slower but more expressive.  \n",
    "- Together, they balance **speed vs. capacity** — a critical trade-off in real-world production RecSys.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Build features ====\n",
    "u_idx = data[\"u_idx\"].astype(\"int32\").values\n",
    "m_idx = data[\"m_idx\"].astype(\"int32\").values\n",
    "\n",
    "ohe_demo = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "X_demo   = ohe_demo.fit_transform(data[['Age','Gender', 'Occupation', 'Zip-code']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2eff68",
   "metadata": {},
   "source": [
    "### 📝 Task 4 – Include additional characteristics to the dataset\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e767be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TO DO 4 ===\n",
    "# Create a multi-label binarizer for movie genres using MultiLabelBinarizer.\n",
    "# Transform the \"Genres_list\" column into a multi-hot encoding and wrap it \n",
    "# with `csr_matrix` for efficiency.\n",
    "\n",
    "from src.task4 import get_wide_input\n",
    "X_wide, X_gen, mlb = get_wide_input(data, X_demo)\n",
    "\n",
    "wide_dim = X_wide.shape[1]\n",
    "\n",
    "print(f\"Users: {n_users}, Items: {n_items}, Wide dims: {wide_dim}, Samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 2) Model: Wide & Deep for ratings ====\n",
    "def create_wide_deep_regression(num_users, num_items, wide_dim, embedding_dim=16):\n",
    "    user_in = Input(shape=(), dtype=\"int32\", name=\"user_id\")\n",
    "    item_in = Input(shape=(), dtype=\"int32\", name=\"item_id\")\n",
    "    wide_in = Input(shape=(wide_dim,), dtype=\"float32\", name=\"wide\")\n",
    "\n",
    "    # Deep branch (embeddings)\n",
    "    u_emb = Embedding(num_users, embedding_dim, name=\"user_emb\")(user_in)\n",
    "    i_emb = Embedding(num_items, embedding_dim, name=\"item_emb\")(item_in)\n",
    "    deep  = Concatenate()([Flatten()(u_emb), Flatten()(i_emb)])\n",
    "    deep  = Dense(64, activation=\"relu\")(deep)\n",
    "    deep  = Dense(32, activation=\"relu\")(deep)\n",
    "\n",
    "    # Merge wide + deep\n",
    "    x = Concatenate()([deep, wide_in])\n",
    "    out = Dense(1, activation=None, name=\"rating\")(x)\n",
    "\n",
    "    model = Model([user_in, item_in, wide_in], out)\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                  loss=\"mse\",\n",
    "                  metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "    return model\n",
    "\n",
    "model = create_wide_deep_regression(n_users, n_items, wide_dim=wide_dim, embedding_dim=16)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344f6c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== 3) Train ====\n",
    "history = model.fit(\n",
    "    x={\"user_id\": u_idx, \"item_id\": m_idx, \"wide\": X_wide.toarray()},\n",
    "    y=y,\n",
    "    batch_size=1024,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_recs = fn.recommend_for_uidx_wide(u_idx=10, data=data, model=model,\n",
    "                              ohe_demo=ohe_demo, mlb=mlb, top_n=5)\n",
    "for mid, title, score in top_recs:\n",
    "    print(f\"{title}  (pred: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5062b",
   "metadata": {},
   "source": [
    "---\n",
    "## **Part 3 – Two-Tower Models** 🏛️🏛️\n",
    "\n",
    "Two-Tower models split representation learning into **two parallel “towers”** — one for users and one for items.  \n",
    "This design lets us **precompute item embeddings** and perform **lightning-fast retrieval** at scale. 🚀\n",
    "\n",
    "---\n",
    "\n",
    "### 🏗️ **Model Structure**\n",
    "\n",
    "**👤 User Tower**\n",
    "- **Inputs**: User ID (embedded), demographics (e.g., age, region), recent interactions (past item IDs, session stats).\n",
    "- **Output**: Dense user vector $\\mathbf{u}_u \\in \\mathbb{R}^d$\n",
    "\n",
    "**🎯 Item Tower**\n",
    "- **Inputs**: Item ID (embedded), metadata (category, price), content features (text embeddings, image CNN features).\n",
    "- **Output**: Dense item vector $\\mathbf{v}_i \\in \\mathbb{R}^d$\n",
    "\n",
    "![Two Towers](images/Tower_Architecture.png)\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 **Why Two-Towers?**\n",
    "\n",
    "| Benefit | Impact |\n",
    "|---------|--------|\n",
    "| ⚡ **Scalability** | Precompute item embeddings → one vector lookup + ANN (Approximate Nearest Neighbor) search → millisecond latency. |\n",
    "| 🧩 **Modularity** | Retrain/update towers independently; plug in new features without retraining the whole model. |\n",
    "| 🎨 **Multimodal** | Easily add text, images, or audio by connecting specialized sub-networks. |\n",
    "| 🆕 **Fresh Content** | Generate embeddings for new items instantly, without retraining the user tower or reindexing everything. |\n",
    "| 🖥️ **Resource-Light** | Inference = a single dot product per candidate; highly parallelizable on GPUs/CPUs. |\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key takeaway:**  \n",
    "Two-Tower models **shine in large-scale retrieval** — they get you from *millions* of candidates down to a *small shortlist* in milliseconds, ready for a second-stage ranking model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574cc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 0) Encoders & Side-Feature Setup\n",
    "# ================================\n",
    "\n",
    "# Existing encoders (user-side)\n",
    "le_gender = LabelEncoder().fit(data[\"Gender\"])\n",
    "le_occ    = LabelEncoder().fit(data[\"Occupation\"])\n",
    "le_zip    = LabelEncoder().fit(data[\"Zip-code\"])\n",
    "\n",
    "data[\"g_idx\"] = le_gender.transform(data[\"Gender\"]).astype(\"int32\")\n",
    "data[\"o_idx\"] = le_occ.transform(data[\"Occupation\"]).astype(\"int32\")\n",
    "data[\"z_idx\"] = le_zip.transform(data[\"Zip-code\"]).astype(\"int32\")\n",
    "\n",
    "# Item genres (already computed earlier)\n",
    "G = X_gen  # csr_matrix with shape (n_samples, n_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TO DO 5 ===\n",
    "# Extract the release year from the \"Title\" column and encode it for use as a side feature.\n",
    "# Steps:\n",
    "# 1. Extract the year (4 digits inside parentheses) and store it in a new column \"Year\".\n",
    "# 2. Use LabelEncoder to convert the \"Year\" column into integer indices.\n",
    "# 3. Save the result in a new column \"y_idx\", which will be used for year embeddings.\n",
    "from src.task5 import get_year_embedding\n",
    "\n",
    "data = get_year_embedding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================\n",
    "# 1) Basic dimension\n",
    "# ==================\n",
    "n_users  = data[\"u_idx\"].nunique()\n",
    "n_items  = data[\"m_idx\"].nunique()\n",
    "n_g      = data[\"g_idx\"].nunique()\n",
    "n_o      = data[\"o_idx\"].nunique()\n",
    "n_z      = data[\"z_idx\"].nunique()\n",
    "n_genres = G.shape[1]\n",
    "n_years  = data[\"y_idx\"].nunique()                               \n",
    "\n",
    "u_idx = data[\"u_idx\"].astype(\"int32\").values\n",
    "m_idx = data[\"m_idx\"].astype(\"int32\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b567f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# 2) Two-Tower\n",
    "# ============\n",
    "class TwoTower(Model):\n",
    "    def __init__(self, n_users, n_items, n_g, n_o, n_z, n_genres, n_years,\n",
    "                 emb_dim=32, side_dim=8, tower_dim=64):\n",
    "        super().__init__()\n",
    "        # ID embeddings\n",
    "        self.user_emb = layers.Embedding(n_users, emb_dim, name=\"user_id_emb\")\n",
    "        self.item_emb = layers.Embedding(n_items, emb_dim, name=\"item_id_emb\")\n",
    "        # user side\n",
    "        self.gender_emb = layers.Embedding(n_g, side_dim, name=\"gender_emb\")\n",
    "        self.occ_emb    = layers.Embedding(n_o, side_dim, name=\"occ_emb\")\n",
    "        self.zip_emb    = layers.Embedding(n_z, side_dim, name=\"zip_emb\")\n",
    "        # item side\n",
    "        self.genre_proj = layers.Dense(side_dim, use_bias=False, name=\"genre_proj\")  # projects multi-hot genre vector\n",
    "        self.year_emb   = layers.Embedding(n_years, side_dim, name=\"year_emb\")       \n",
    "\n",
    "        # projections to common tower space\n",
    "        self.user_proj = layers.Dense(tower_dim, activation=None, name=\"user_proj\")\n",
    "        self.item_proj = layers.Dense(tower_dim, activation=None, name=\"item_proj\")\n",
    "\n",
    "    def user_tower(self, user_id, g_idx, o_idx, z_idx):\n",
    "        u = self.user_emb(user_id)\n",
    "        g = self.gender_emb(g_idx)\n",
    "        o = self.occ_emb(o_idx)\n",
    "        z = self.zip_emb(z_idx)\n",
    "        u_cat = tf.concat([u, g, o, z], axis=-1)          # (..., 32 + 3*8 = 56)\n",
    "        return self.user_proj(u_cat)                      # -> (..., tower_dim)\n",
    "\n",
    "    def item_tower(self, item_id, genres_vec, y_idx):\n",
    "        i = self.item_emb(item_id)\n",
    "        g_emb = self.genre_proj(genres_vec)               # (..., 8)\n",
    "        y_emb = self.year_emb(y_idx)                      # (..., 8)   \n",
    "        v_cat = tf.concat([i, g_emb, y_emb], axis=-1)     # (..., 32 + 8 + 8 = 48)\n",
    "        return self.item_proj(v_cat)                      # -> (..., tower_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        u_vec = self.user_tower(inputs[\"user_id\"], inputs[\"g_idx\"], inputs[\"o_idx\"], inputs[\"z_idx\"])\n",
    "        v_vec = self.item_tower(inputs[\"item_id\"], inputs[\"genres\"], inputs[\"y_idx\"])  # NEW\n",
    "        return tf.reduce_sum(u_vec * v_vec, axis=-1, keepdims=True)  # dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1ff45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 3) tf.data generator & signature\n",
    "# =========================================\n",
    "def _as_float1d(x):\n",
    "    # Accepts dense row or scipy sparse row\n",
    "    if hasattr(x, \"toarray\"):\n",
    "        return x.toarray().astype(\"float32\").ravel()\n",
    "    return np.asarray(x, dtype=\"float32\").ravel()\n",
    "\n",
    "def gen():\n",
    "    for i in range(len(data)):\n",
    "        yield (\n",
    "            {\n",
    "                \"user_id\": np.int32(u_idx[i]),\n",
    "                \"item_id\": np.int32(m_idx[i]),\n",
    "                \"g_idx\":   np.int32(data[\"g_idx\"].iloc[i]),\n",
    "                \"o_idx\":   np.int32(data[\"o_idx\"].iloc[i]),\n",
    "                \"z_idx\":   np.int32(data[\"z_idx\"].iloc[i]),\n",
    "                \"genres\":  _as_float1d(G[i]),               # (n_genres,) float32\n",
    "                \"y_idx\":   np.int32(data[\"y_idx\"].iloc[i]),\n",
    "            },\n",
    "            np.float32(y[i]),\n",
    "        )\n",
    "\n",
    "sig = (\n",
    "    {\n",
    "        \"user_id\": tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        \"item_id\": tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        \"g_idx\":   tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        \"o_idx\":   tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        \"z_idx\":   tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        \"genres\":  tf.TensorSpec(shape=(n_genres,), dtype=tf.float32),\n",
    "        \"y_idx\":   tf.TensorSpec(shape=(), dtype=tf.int32),        \n",
    "    },\n",
    "    tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    ")\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(gen, output_signature=sig).batch(1024).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 4) Compile, Train, Evaluate\n",
    "# ===========================\n",
    "model = TwoTower(n_users, n_items, n_g, n_o, n_z, n_genres, n_years, emb_dim=32, side_dim=8, tower_dim=64)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")])\n",
    "model.fit(ds, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 5) Inference / Recommendation\n",
    "# ===========================\n",
    "# Example usage:\n",
    "top_recs = fn.recommend_for_uidx_tt(10, data, model, mlb, top_n=5)\n",
    "print(top_recs[[\"Title\", \"pred\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a79fc",
   "metadata": {},
   "source": [
    "# [🏆 **Factored Tech Week 2025 – Recommender Systems Challenge**](https://www.codabench.org/competitions/10195/?secret_key=a03669ef-a8e7-483a-aa8b-3686beb4be9b)\n",
    "\n",
    "![Factored Banner](images/Factored_Logo_Profile_Asset_Cover-.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
